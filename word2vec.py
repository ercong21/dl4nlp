# -*- coding: utf-8 -*-
"""word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_EOI2ztqiOfnBZI78x3Rqv2WDquMJiag

This file is the implementation of the **word2vec** embedding model. **Brown corpus** is used as an example for training.
"""

# import brwon corpus from nltk library
from nltk.corpus import brown
import nltk
nltk.download('brown')
corpus = brown.words()  # contains 1,161,192 tokens

# set the vocabulary size as 10,000
# create a dictionary mapping the most frequent 10,000 words with their word index

from collections import Counter
VOCAB_SIZE = 10000

counter = Counter(corpus)
vocab = counter.most_common(VOCAB_SIZE)
word2id = {w[0]:n for n, w in enumerate(vocab)}
id2word = {n:w[0] for n, w in enumerate(vocab)}

# create the training dataset from the corpus
import random
from tqdm import tqdm

# set window size as 2, number of negative samples as 10
WINDOW_SIZE = 2
NEG_SAMPLES_FACTORS = 10

train_set = list()
tokens = list(corpus)

for i, f_mid_token in tqdm(enumerate(tokens)):
  if f_mid_token in word2id:
    context_words = tokens[i+1:i+1+WINDOW_SIZE]
    f_mid_token_id = word2id[f_mid_token]
    for context_word in context_words:
      if context_word in word2id:
        context_word_id = word2id[context_word]
        train_set.append((f_mid_token_id, context_word_id, True))
        for i in range(NEG_SAMPLES_FACTORS):
          false_context_id = random.randint(0, VOCAB_SIZE-1)
          if false_context_id != context_word_id:
            train_set.append((f_mid_token_id, false_context_id, False))
  
  b_mid_token = corpus[i-VOCAB_SIZE]
  if b_mid_token in word2id:
    b_mid_token_id = word2id[b_mid_token]
    context_words = tokens[i-VOCAB_SIZE - WINDOW_SIZE: i-VOCAB_SIZE]
    for context_word in context_words:
      if context_word in word2id:
        context_word_id = word2id[context_word]
        train_set.append((b_mid_token_id, context_word_id, True))
        for i in range(NEG_SAMPLES_FACTORS):
          false_context_id = random.randint(0, VOCAB_SIZE-1)
          if false_context_id != context_word_id:
            train_set.append((b_mid_token_id, false_context_id, False))

# initialize the orginal embeddings by normal distribution

import numpy as np

# set the word embedding dimension as 50
NUM_DIM = 50
target_word_matrix = 0.1 * np.random.randn(VOCAB_SIZE, NUM_DIM)
context_word_matrix = 0.1 * np.random.randn(VOCAB_SIZE, NUM_DIM)

# define a function to calculate the sigmoid of the dot product of two vectors

import math

def sigmoid(x):
  dot_prod = vec1.dot(vec2)
  return 1/(1+math.exp(-dot_prod))

# updating the word embedding matrix
# define a function for updating

def update(target_id, context_id, label, lr):
  context_vec = context_word_matrix[context_id]
  target_vec = target_word_matrix[target_id]
  prob = sigmoid(context_vec, target_vec)
  new_vec_ctxt = context_vec + lr * (label - prob) * target_vec
  new_vec_tgt = target_vec + lr * (label-prob) * context_vec
  # update the embedding matrix
  context_word_matrix[context_id] = new_vec_ctxt
  target_word_matrix[target_id] = new_vec_tgt

  log_likelihood = math.log(prob) if label else math.log(1-prob)
  return log_likelihood

# set the number of epoch as 2
EPOCH = 2
LR = 0.1  # set the learning rate as 0.1

for _ in range(EPOCH):
  random.shuffle(train_set)
  log_likelihood = 0
  for tgt_id, ctxt_id, label in tqdm(train_set):
    log_likelihood += update(tgt_id, ctxt_id, label, LR)
  print("Log Likelihood: %.4f" % (log_likelihood))

# define a function to calculate the word similarity

def word_similarity(w1, w2):
  if not (w1 in word2id and w2 in word2id):
    return 0
  w_id1, w_id2 = word2id[w1], word2id[w2]
  vec1, vec2 = target_word_matrix[w_id1], target_word_matrix[w_id2]
  dot12, dot11, dot22 = vec1.dot(vec2.T), vec1.dot(vec1), vec2.dot(vec2)
  return dot12 / math.sqrt(dot11 * dot22)

# define a function to find the most similar words of a given word

def most_similar_words(word, n):
  if word not in word2id:
    return []
  w_id = word2id[word]
  vec =target_word_matrix[w_id, :]
  m = target_word_matrix
  dot_m_v = m.dot(vec.T) # n-dim vector
  dot_m_m = np.sum(m * m, axis=1) # n-dim vector
  dot_v_v = vec.dot(vec.T) # float
  sims = dot_m_v / (math.sqrt(dot_v_v) * np.sqrt(dot_m_m))
  return [id2word[idx] for idx in (-sims).argsort()[:n]]

print(most_similar_words('mother', 10))

